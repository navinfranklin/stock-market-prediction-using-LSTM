{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMNw-kNGxwOE"
   },
   "source": [
    "# stock market prediction using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-1dvAu-lv8-"
   },
   "source": [
    "# for data collection we have used pandas data reader which is a inbuild feature in pandas ..i have not mentioned api key for security issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36kbH8-nUxME"
   },
   "outputs": [],
   "source": [
    "import pandas_datareader as pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBTkmWOlXD-L"
   },
   "outputs": [],
   "source": [
    "df = pdr.get_data_tiingo('AAPL', api_key= \"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwgDpmAFmURP"
   },
   "source": [
    "# after we have imported the data from the tiingo api we need to convert into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNmVqeG4Xhbc"
   },
   "outputs": [],
   "source": [
    "df.to_csv('AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPsHsezGYTfF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4_je9CdYZsi"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('AAPL.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAw79-TBm1F3"
   },
   "source": [
    "# exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "EfN2ZrPTYfQU",
    "outputId": "6006c5a9-2438-4174-f6c0-2128fb0344ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjClose</th>\n",
       "      <th>adjHigh</th>\n",
       "      <th>adjLow</th>\n",
       "      <th>adjOpen</th>\n",
       "      <th>adjVolume</th>\n",
       "      <th>divCash</th>\n",
       "      <th>splitFactor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-11-03 00:00:00+00:00</td>\n",
       "      <td>122.57</td>\n",
       "      <td>123.49</td>\n",
       "      <td>120.70</td>\n",
       "      <td>120.79</td>\n",
       "      <td>45518976</td>\n",
       "      <td>28.313084</td>\n",
       "      <td>28.525599</td>\n",
       "      <td>27.881122</td>\n",
       "      <td>27.901912</td>\n",
       "      <td>182075904</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-11-04 00:00:00+00:00</td>\n",
       "      <td>122.00</td>\n",
       "      <td>123.82</td>\n",
       "      <td>121.62</td>\n",
       "      <td>123.13</td>\n",
       "      <td>44886050</td>\n",
       "      <td>28.181416</td>\n",
       "      <td>28.601828</td>\n",
       "      <td>28.093638</td>\n",
       "      <td>28.442441</td>\n",
       "      <td>179544200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-11-05 00:00:00+00:00</td>\n",
       "      <td>120.92</td>\n",
       "      <td>122.69</td>\n",
       "      <td>120.18</td>\n",
       "      <td>121.85</td>\n",
       "      <td>39552680</td>\n",
       "      <td>28.052059</td>\n",
       "      <td>28.462679</td>\n",
       "      <td>27.880387</td>\n",
       "      <td>28.267808</td>\n",
       "      <td>158210720</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-11-06 00:00:00+00:00</td>\n",
       "      <td>121.06</td>\n",
       "      <td>121.81</td>\n",
       "      <td>120.62</td>\n",
       "      <td>121.11</td>\n",
       "      <td>33042283</td>\n",
       "      <td>28.084537</td>\n",
       "      <td>28.258529</td>\n",
       "      <td>27.982462</td>\n",
       "      <td>28.096137</td>\n",
       "      <td>132169132</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-11-09 00:00:00+00:00</td>\n",
       "      <td>120.57</td>\n",
       "      <td>121.81</td>\n",
       "      <td>120.05</td>\n",
       "      <td>120.96</td>\n",
       "      <td>33871405</td>\n",
       "      <td>27.970863</td>\n",
       "      <td>28.258529</td>\n",
       "      <td>27.850229</td>\n",
       "      <td>28.061338</td>\n",
       "      <td>135485620</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol                       date   close  ...  adjVolume  divCash  splitFactor\n",
       "0   AAPL  2015-11-03 00:00:00+00:00  122.57  ...  182075904     0.00          1.0\n",
       "1   AAPL  2015-11-04 00:00:00+00:00  122.00  ...  179544200     0.00          1.0\n",
       "2   AAPL  2015-11-05 00:00:00+00:00  120.92  ...  158210720     0.52          1.0\n",
       "3   AAPL  2015-11-06 00:00:00+00:00  121.06  ...  132169132     0.00          1.0\n",
       "4   AAPL  2015-11-09 00:00:00+00:00  120.57  ...  135485620     0.00          1.0\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "DhiC3OuaY4W5",
    "outputId": "8fa9b43d-15bd-4a61-b678-ce3543abd678"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjClose</th>\n",
       "      <th>adjHigh</th>\n",
       "      <th>adjLow</th>\n",
       "      <th>adjOpen</th>\n",
       "      <th>adjVolume</th>\n",
       "      <th>divCash</th>\n",
       "      <th>splitFactor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-10-26 00:00:00+00:00</td>\n",
       "      <td>115.05</td>\n",
       "      <td>116.55</td>\n",
       "      <td>112.8800</td>\n",
       "      <td>114.01</td>\n",
       "      <td>111850657</td>\n",
       "      <td>115.05</td>\n",
       "      <td>116.55</td>\n",
       "      <td>112.8800</td>\n",
       "      <td>114.01</td>\n",
       "      <td>111850657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-10-27 00:00:00+00:00</td>\n",
       "      <td>116.60</td>\n",
       "      <td>117.28</td>\n",
       "      <td>114.5399</td>\n",
       "      <td>115.49</td>\n",
       "      <td>92276772</td>\n",
       "      <td>116.60</td>\n",
       "      <td>117.28</td>\n",
       "      <td>114.5399</td>\n",
       "      <td>115.49</td>\n",
       "      <td>92276772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-10-28 00:00:00+00:00</td>\n",
       "      <td>111.20</td>\n",
       "      <td>115.43</td>\n",
       "      <td>111.1000</td>\n",
       "      <td>115.05</td>\n",
       "      <td>143937823</td>\n",
       "      <td>111.20</td>\n",
       "      <td>115.43</td>\n",
       "      <td>111.1000</td>\n",
       "      <td>115.05</td>\n",
       "      <td>143937823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-10-29 00:00:00+00:00</td>\n",
       "      <td>115.32</td>\n",
       "      <td>116.93</td>\n",
       "      <td>112.2000</td>\n",
       "      <td>112.37</td>\n",
       "      <td>146129173</td>\n",
       "      <td>115.32</td>\n",
       "      <td>116.93</td>\n",
       "      <td>112.2000</td>\n",
       "      <td>112.37</td>\n",
       "      <td>146129173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020-10-30 00:00:00+00:00</td>\n",
       "      <td>108.86</td>\n",
       "      <td>111.99</td>\n",
       "      <td>107.7200</td>\n",
       "      <td>111.06</td>\n",
       "      <td>190573476</td>\n",
       "      <td>108.86</td>\n",
       "      <td>111.99</td>\n",
       "      <td>107.7200</td>\n",
       "      <td>111.06</td>\n",
       "      <td>190573476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     symbol                       date   close  ...  adjVolume  divCash  splitFactor\n",
       "1253   AAPL  2020-10-26 00:00:00+00:00  115.05  ...  111850657      0.0          1.0\n",
       "1254   AAPL  2020-10-27 00:00:00+00:00  116.60  ...   92276772      0.0          1.0\n",
       "1255   AAPL  2020-10-28 00:00:00+00:00  111.20  ...  143937823      0.0          1.0\n",
       "1256   AAPL  2020-10-29 00:00:00+00:00  115.32  ...  146129173      0.0          1.0\n",
       "1257   AAPL  2020-10-30 00:00:00+00:00  108.86  ...  190573476      0.0          1.0\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY1PvFrporfp"
   },
   "source": [
    "# remove the index value and take the indepentent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E23N4zKuY_pI"
   },
   "outputs": [],
   "source": [
    "df1=df.reset_index()['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dt-qckmmZGzW",
    "outputId": "5e57e37b-4af1-447e-f342-382066f0c8a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       122.57\n",
       "1       122.00\n",
       "2       120.92\n",
       "3       121.06\n",
       "4       120.57\n",
       "         ...  \n",
       "1253    115.05\n",
       "1254    116.60\n",
       "1255    111.20\n",
       "1256    115.32\n",
       "1257    108.86\n",
       "Name: close, Length: 1258, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czpl0UrnZK6Y"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z52lyz-tpQTL"
   },
   "source": [
    "# for LSTM we have to make the values small for good results as the lstm model is more sensitive so we will be using minmaxscalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMDVlNK-Z34W"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "df1=scaler.fit_transform(np.array(df1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAOObCaPakNG",
    "outputId": "ccd7c044-adc1-40d0-fe62-cd7d85495a50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07752255],\n",
       "       [0.07615153],\n",
       "       [0.07355382],\n",
       "       ...,\n",
       "       [0.05017438],\n",
       "       [0.06008419],\n",
       "       [0.044546  ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0iIXlHkpi0M"
   },
   "source": [
    "# as this dataset is a time series data and each of the values is dependent on the pervious values so we cannot randomly split the data using normal train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keDq7sCxaxjp"
   },
   "outputs": [],
   "source": [
    "training_size=int(len(df1)*0.65)\n",
    "test_size=len(df1)-training_size\n",
    "train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzpNlmkKbZdc",
    "outputId": "f20068c3-6bf9-4666-a4b3-e7bfa54ef361"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(817, 441)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size,test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4PgAv_nbpRU",
    "outputId": "ab6c56b3-5e0e-42bf-e7a7-38ba679175ba"
   },
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RJLUAZFp7Gf"
   },
   "source": [
    "# now we have to convert the array values into matrix and most importantly the values of i will be starting from 0,1,2,3....99 as we have used 100 as we have mentioned the timestep 100\n",
    "example if there is 1 2 3 4 ouput 5 this value 5 is dependent on the other 4 values on thee time  series data ...now as we give time stamp as 100 it we check the value before predicting the output 100 values before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIAxuu8_bu-S"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "def create_dataset(dataset, time_step=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-time_step-1):\n",
    "\t\ta = dataset[i:(i+time_step), 0]   \n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + time_step, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VTIaUVQb2wn"
   },
   "outputs": [],
   "source": [
    "time_step = 100\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, ytest = create_dataset(test_data, time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjfFIY4LcN6m",
    "outputId": "bd1d7df0-d891-41d8-8136-4402f153fbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(716, 100)\n",
      "(716,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape), print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QplBlmH1cWbP",
    "outputId": "693e7a39-682a-43e7-ccf1-9324b78575b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340, 100)\n",
      "(340,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape), print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtD7ECTctbCq"
   },
   "source": [
    "# before implementing any LSTM model we have to convert the data dimensions into a 3d dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qe6TeDtbcaAa"
   },
   "outputs": [],
   "source": [
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IzpZRlcdmZy"
   },
   "source": [
    "# create a LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HanMeLadArq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkdmxxczdkkA"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9IHPiGBege0",
    "outputId": "0b18a900-78b7-4fb5-8ea0-e79b3898dbf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 50)           10400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 50)           20200     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 50,851\n",
      "Trainable params: 50,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRYO0T5tem-O",
    "outputId": "0d07448e-1a9c-46b1-a225-0970048daa6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 3s 268ms/step - loss: 0.0076 - val_loss: 0.0556\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 0.0017 - val_loss: 0.0216\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 5.2908e-04 - val_loss: 0.0223\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 4.1503e-04 - val_loss: 0.0204\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 3.6073e-04 - val_loss: 0.0201\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 3.3616e-04 - val_loss: 0.0195\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 3.2638e-04 - val_loss: 0.0189\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 3.1668e-04 - val_loss: 0.0184\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 3.1923e-04 - val_loss: 0.0180\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 2.9374e-04 - val_loss: 0.0175\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 2.9229e-04 - val_loss: 0.0169\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 2.7050e-04 - val_loss: 0.0164\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 2.7191e-04 - val_loss: 0.0163\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 2.4898e-04 - val_loss: 0.0163\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 2.5707e-04 - val_loss: 0.0155\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 2.3204e-04 - val_loss: 0.0149\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 2.2142e-04 - val_loss: 0.0156\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 2.3310e-04 - val_loss: 0.0146\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 2.5802e-04 - val_loss: 0.0147\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 1.9721e-04 - val_loss: 0.0146\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 1.9748e-04 - val_loss: 0.0155\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 2.1751e-04 - val_loss: 0.0144\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 2.2415e-04 - val_loss: 0.0150\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 2.1517e-04 - val_loss: 0.0142\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 1.8661e-04 - val_loss: 0.0142\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 1.8111e-04 - val_loss: 0.0142\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 1.8090e-04 - val_loss: 0.0141\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 2.1390e-04 - val_loss: 0.0142\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.8979e-04 - val_loss: 0.0139\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 1.9061e-04 - val_loss: 0.0139\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 1.8667e-04 - val_loss: 0.0141\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 1.8434e-04 - val_loss: 0.0137\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 1.8112e-04 - val_loss: 0.0144\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 1.9797e-04 - val_loss: 0.0140\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 2.1241e-04 - val_loss: 0.0143\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 1.9306e-04 - val_loss: 0.0134\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 1.6667e-04 - val_loss: 0.0135\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 1.7417e-04 - val_loss: 0.0137\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 1.8267e-04 - val_loss: 0.0134\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 1.7782e-04 - val_loss: 0.0133\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 1.5796e-04 - val_loss: 0.0132\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 1.5925e-04 - val_loss: 0.0130\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 1.5596e-04 - val_loss: 0.0130\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 1.6290e-04 - val_loss: 0.0130\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 1.6338e-04 - val_loss: 0.0132\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 1.5680e-04 - val_loss: 0.0131\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 1.5383e-04 - val_loss: 0.0131\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 1.8825e-04 - val_loss: 0.0129\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.5247e-04 - val_loss: 0.0123\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 1.5689e-04 - val_loss: 0.0123\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 1.6508e-04 - val_loss: 0.0122\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 1.4658e-04 - val_loss: 0.0123\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 1.4296e-04 - val_loss: 0.0121\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 1.5105e-04 - val_loss: 0.0120\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 1.4106e-04 - val_loss: 0.0119\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 1.5293e-04 - val_loss: 0.0118\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 1.4079e-04 - val_loss: 0.0116\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 1.4487e-04 - val_loss: 0.0113\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 1.3598e-04 - val_loss: 0.0113\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 1.4008e-04 - val_loss: 0.0112\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 1.3767e-04 - val_loss: 0.0111\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.5142e-04 - val_loss: 0.0108\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 1.4846e-04 - val_loss: 0.0107\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 1.4021e-04 - val_loss: 0.0105\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 1.3423e-04 - val_loss: 0.0103\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 1.2933e-04 - val_loss: 0.0102\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 1.3183e-04 - val_loss: 0.0101\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 1.2977e-04 - val_loss: 0.0098\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 1.3854e-04 - val_loss: 0.0101\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 1.4597e-04 - val_loss: 0.0096\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.3776e-04 - val_loss: 0.0094\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.2813e-04 - val_loss: 0.0093\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 1.1799e-04 - val_loss: 0.0092\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.2787e-04 - val_loss: 0.0091\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 1.2058e-04 - val_loss: 0.0090\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 1.2346e-04 - val_loss: 0.0091\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 1.1504e-04 - val_loss: 0.0090\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 1.2238e-04 - val_loss: 0.0087\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.1865e-04 - val_loss: 0.0088\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 1.2932e-04 - val_loss: 0.0084\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.2775e-04 - val_loss: 0.0083\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 1.2504e-04 - val_loss: 0.0087\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 1.1870e-04 - val_loss: 0.0081\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 1.1494e-04 - val_loss: 0.0085\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 1.2804e-04 - val_loss: 0.0079\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 1.1103e-04 - val_loss: 0.0079\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 1.2379e-04 - val_loss: 0.0084\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.2266e-04 - val_loss: 0.0081\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 1.0948e-04 - val_loss: 0.0081\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 1.1367e-04 - val_loss: 0.0075\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.3153e-04 - val_loss: 0.0080\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 1.3381e-04 - val_loss: 0.0075\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 1.0423e-04 - val_loss: 0.0078\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 1.0834e-04 - val_loss: 0.0074\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 2s 205ms/step - loss: 1.0313e-04 - val_loss: 0.0072\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 1.0001e-04 - val_loss: 0.0074\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 1.0062e-04 - val_loss: 0.0087\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.3952e-04 - val_loss: 0.0070\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.2207e-04 - val_loss: 0.0070\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 1.0351e-04 - val_loss: 0.0069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd77882ef28>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuNxMyDEgKnq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdfFY0BtujDJ"
   },
   "source": [
    "# now we have to predict the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnXpXdkigPXk"
   },
   "outputs": [],
   "source": [
    "train_predict=model.predict(X_train)\n",
    "test_predict=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2qhyd8Pun2X"
   },
   "source": [
    "# as we have converted into small values using minmaxscaler we are converting back into the original values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMQKWXRagWyH"
   },
   "outputs": [],
   "source": [
    "train_predict=scaler.inverse_transform(train_predict)\n",
    "test_predict=scaler.inverse_transform(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uii_Cnjzu8Ca"
   },
   "source": [
    "# to check wheather our LSTM model have done good we have the check the mean square error value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQsZTFsggbUV",
    "outputId": "48c42da9-b7d5-4a43-9345-5f7e84394d42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156.25532480710353"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "math.sqrt(mean_squared_error(y_train,train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R34iFm8kgiKi",
    "outputId": "4467765d-645d-49d5-cb0a-dfc344ae3dfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276.8269867575707"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(mean_squared_error(ytest,test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MmnjYxXv3cK"
   },
   "source": [
    "# for plotting train predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyQMj7R4glXL"
   },
   "outputs": [],
   "source": [
    "look_back=100\n",
    "trainPredictPlot = numpy.empty_like(df1)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUTMF7RQv-Mb"
   },
   "source": [
    "# for plotting test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfQ3NaPkg9Er"
   },
   "outputs": [],
   "source": [
    "testPredictPlot = numpy.empty_like(df1)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXAwH8BCwC4S"
   },
   "source": [
    "# plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "NRPeMneXhBB5",
    "outputId": "1262cb80-c114-4630-dc17-b5c09b468553"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scaler.inverse_transform(df1))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8TA1SLVlAGi",
    "outputId": "d87a5096-a906-4472-f75f-86e4243e117f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkDSzF22w_8I",
    "outputId": "ba0e2503-8442-4206-8cf6-80ad0a8bde0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input=test_data[341:].reshape(1,-1)\n",
    "x_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Dj5WcFoxC2Y"
   },
   "outputs": [],
   "source": [
    "temp_input=list(x_input)\n",
    "temp_input=temp_input[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cd2e462axKqA",
    "outputId": "dc51f1b7-311c-4f3b-bc92-18b41c889fa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5906434155141311,\n",
       " 0.5976187612748045,\n",
       " 0.607696933253157,\n",
       " 0.6295610342754059,\n",
       " 0.6283824413710162,\n",
       " 0.628719182200842,\n",
       " 0.6238845460012028,\n",
       " 0.6458929645219482,\n",
       " 0.6643174984966926,\n",
       " 0.6487552615754661,\n",
       " 0.6602525556223691,\n",
       " 0.6332892363199039,\n",
       " 0.6528923631990378,\n",
       " 0.6601563439567049,\n",
       " 0.6584966927239928,\n",
       " 0.6584966927239928,\n",
       " 0.6819242333132893,\n",
       " 0.6791340950090199,\n",
       " 0.7000120264582081,\n",
       " 0.7039567047504509,\n",
       " 0.7055682501503309,\n",
       " 0.7013108839446784,\n",
       " 0.7165123271196634,\n",
       " 0.7229344558027659,\n",
       " 0.7113650030066145,\n",
       " 0.7094888755261575,\n",
       " 0.7290198436560433,\n",
       " 0.7159591100420926,\n",
       " 0.7185808779314491,\n",
       " 0.6759831629585087,\n",
       " 0.673770294648226,\n",
       " 0.6948887552615755,\n",
       " 0.6799037883343355,\n",
       " 0.6971016235718581,\n",
       " 0.7081659651232712,\n",
       " 0.8050511124473845,\n",
       " 0.830811785929044,\n",
       " 0.8378111846061336,\n",
       " 0.841635598316296,\n",
       " 0.8785808779314492,\n",
       " 0.8517378232110644,\n",
       " 0.8672760072158752,\n",
       " 0.8350210463018641,\n",
       " 0.8699939867708961,\n",
       " 0.8892363199037885,\n",
       " 0.8882501503307276,\n",
       " 0.8853638003607938,\n",
       " 0.89455201443175,\n",
       " 0.8959470835838845,\n",
       " 0.9206494287432352,\n",
       " 0.9792904389657247,\n",
       " 0.9936019242333134,\n",
       " 0.9836680697534577,\n",
       " 1.0,\n",
       " 0.9854479855682503,\n",
       " 0.983499699338545,\n",
       " 0.09308478653036678,\n",
       " 0.10544798556825014,\n",
       " 0.09876127480457003,\n",
       " 0.07345760673481655,\n",
       " 0.07365003006614551,\n",
       " 0.05407095610342749,\n",
       " 0.06489476849067943,\n",
       " 0.055682501503307225,\n",
       " 0.052098616957306054,\n",
       " 0.0601683704149128,\n",
       " 0.06061334936861096,\n",
       " 0.05241130487071555,\n",
       " 0.04810583283223091,\n",
       " 0.03968731208659049,\n",
       " 0.04748045700541187,\n",
       " 0.05164161154539987,\n",
       " 0.04036079374624174,\n",
       " 0.04300661455201443,\n",
       " 0.05277209861695731,\n",
       " 0.05921828021647624,\n",
       " 0.05712567648827421,\n",
       " 0.06126277811184605,\n",
       " 0.06361996392062536,\n",
       " 0.05455201443174984,\n",
       " 0.062922429344558,\n",
       " 0.05488875526157544,\n",
       " 0.059506915213469624,\n",
       " 0.059242333132892344,\n",
       " 0.06405291641611544,\n",
       " 0.08192423331328924,\n",
       " 0.07398677089597111,\n",
       " 0.07420324714371618,\n",
       " 0.07304870715574258,\n",
       " 0.06898376428141911,\n",
       " 0.061671677690920024,\n",
       " 0.06535177390258567,\n",
       " 0.06381238725195432,\n",
       " 0.06111846061334936,\n",
       " 0.059410703547805144,\n",
       " 0.05943475646422125,\n",
       " 0.06316295850871917,\n",
       " 0.05017438364401683,\n",
       " 0.06008418520745634,\n",
       " 0.04454600120264579]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMXmfYcqxTbm"
   },
   "source": [
    "##predicting for next 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D06Kog20xOZn",
    "outputId": "a109e82c-79a7-4d32-bac5-28e2d6750c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05345364]\n",
      "101\n",
      "1 day input [0.59761876 0.60769693 0.62956103 0.62838244 0.62871918 0.62388455\n",
      " 0.64589296 0.6643175  0.64875526 0.66025256 0.63328924 0.65289236\n",
      " 0.66015634 0.65849669 0.65849669 0.68192423 0.6791341  0.70001203\n",
      " 0.7039567  0.70556825 0.70131088 0.71651233 0.72293446 0.711365\n",
      " 0.70948888 0.72901984 0.71595911 0.71858088 0.67598316 0.67377029\n",
      " 0.69488876 0.67990379 0.69710162 0.70816597 0.80505111 0.83081179\n",
      " 0.83781118 0.8416356  0.87858088 0.85173782 0.86727601 0.83502105\n",
      " 0.86999399 0.88923632 0.88825015 0.8853638  0.89455201 0.89594708\n",
      " 0.92064943 0.97929044 0.99360192 0.98366807 1.         0.98544799\n",
      " 0.9834997  0.09308479 0.10544799 0.09876127 0.07345761 0.07365003\n",
      " 0.05407096 0.06489477 0.0556825  0.05209862 0.06016837 0.06061335\n",
      " 0.0524113  0.04810583 0.03968731 0.04748046 0.05164161 0.04036079\n",
      " 0.04300661 0.0527721  0.05921828 0.05712568 0.06126278 0.06361996\n",
      " 0.05455201 0.06292243 0.05488876 0.05950692 0.05924233 0.06405292\n",
      " 0.08192423 0.07398677 0.07420325 0.07304871 0.06898376 0.06167168\n",
      " 0.06535177 0.06381239 0.06111846 0.0594107  0.05943476 0.06316296\n",
      " 0.05017438 0.06008419 0.044546   0.05345364]\n",
      "1 day output [[0.05219891]]\n",
      "2 day input [0.60769693 0.62956103 0.62838244 0.62871918 0.62388455 0.64589296\n",
      " 0.6643175  0.64875526 0.66025256 0.63328924 0.65289236 0.66015634\n",
      " 0.65849669 0.65849669 0.68192423 0.6791341  0.70001203 0.7039567\n",
      " 0.70556825 0.70131088 0.71651233 0.72293446 0.711365   0.70948888\n",
      " 0.72901984 0.71595911 0.71858088 0.67598316 0.67377029 0.69488876\n",
      " 0.67990379 0.69710162 0.70816597 0.80505111 0.83081179 0.83781118\n",
      " 0.8416356  0.87858088 0.85173782 0.86727601 0.83502105 0.86999399\n",
      " 0.88923632 0.88825015 0.8853638  0.89455201 0.89594708 0.92064943\n",
      " 0.97929044 0.99360192 0.98366807 1.         0.98544799 0.9834997\n",
      " 0.09308479 0.10544799 0.09876127 0.07345761 0.07365003 0.05407096\n",
      " 0.06489477 0.0556825  0.05209862 0.06016837 0.06061335 0.0524113\n",
      " 0.04810583 0.03968731 0.04748046 0.05164161 0.04036079 0.04300661\n",
      " 0.0527721  0.05921828 0.05712568 0.06126278 0.06361996 0.05455201\n",
      " 0.06292243 0.05488876 0.05950692 0.05924233 0.06405292 0.08192423\n",
      " 0.07398677 0.07420325 0.07304871 0.06898376 0.06167168 0.06535177\n",
      " 0.06381239 0.06111846 0.0594107  0.05943476 0.06316296 0.05017438\n",
      " 0.06008419 0.044546   0.05345364 0.05219891]\n",
      "2 day output [[0.05118345]]\n",
      "3 day input [0.62956103 0.62838244 0.62871918 0.62388455 0.64589296 0.6643175\n",
      " 0.64875526 0.66025256 0.63328924 0.65289236 0.66015634 0.65849669\n",
      " 0.65849669 0.68192423 0.6791341  0.70001203 0.7039567  0.70556825\n",
      " 0.70131088 0.71651233 0.72293446 0.711365   0.70948888 0.72901984\n",
      " 0.71595911 0.71858088 0.67598316 0.67377029 0.69488876 0.67990379\n",
      " 0.69710162 0.70816597 0.80505111 0.83081179 0.83781118 0.8416356\n",
      " 0.87858088 0.85173782 0.86727601 0.83502105 0.86999399 0.88923632\n",
      " 0.88825015 0.8853638  0.89455201 0.89594708 0.92064943 0.97929044\n",
      " 0.99360192 0.98366807 1.         0.98544799 0.9834997  0.09308479\n",
      " 0.10544799 0.09876127 0.07345761 0.07365003 0.05407096 0.06489477\n",
      " 0.0556825  0.05209862 0.06016837 0.06061335 0.0524113  0.04810583\n",
      " 0.03968731 0.04748046 0.05164161 0.04036079 0.04300661 0.0527721\n",
      " 0.05921828 0.05712568 0.06126278 0.06361996 0.05455201 0.06292243\n",
      " 0.05488876 0.05950692 0.05924233 0.06405292 0.08192423 0.07398677\n",
      " 0.07420325 0.07304871 0.06898376 0.06167168 0.06535177 0.06381239\n",
      " 0.06111846 0.0594107  0.05943476 0.06316296 0.05017438 0.06008419\n",
      " 0.044546   0.05345364 0.05219891 0.05118345]\n",
      "3 day output [[0.0504397]]\n",
      "4 day input [0.62838244 0.62871918 0.62388455 0.64589296 0.6643175  0.64875526\n",
      " 0.66025256 0.63328924 0.65289236 0.66015634 0.65849669 0.65849669\n",
      " 0.68192423 0.6791341  0.70001203 0.7039567  0.70556825 0.70131088\n",
      " 0.71651233 0.72293446 0.711365   0.70948888 0.72901984 0.71595911\n",
      " 0.71858088 0.67598316 0.67377029 0.69488876 0.67990379 0.69710162\n",
      " 0.70816597 0.80505111 0.83081179 0.83781118 0.8416356  0.87858088\n",
      " 0.85173782 0.86727601 0.83502105 0.86999399 0.88923632 0.88825015\n",
      " 0.8853638  0.89455201 0.89594708 0.92064943 0.97929044 0.99360192\n",
      " 0.98366807 1.         0.98544799 0.9834997  0.09308479 0.10544799\n",
      " 0.09876127 0.07345761 0.07365003 0.05407096 0.06489477 0.0556825\n",
      " 0.05209862 0.06016837 0.06061335 0.0524113  0.04810583 0.03968731\n",
      " 0.04748046 0.05164161 0.04036079 0.04300661 0.0527721  0.05921828\n",
      " 0.05712568 0.06126278 0.06361996 0.05455201 0.06292243 0.05488876\n",
      " 0.05950692 0.05924233 0.06405292 0.08192423 0.07398677 0.07420325\n",
      " 0.07304871 0.06898376 0.06167168 0.06535177 0.06381239 0.06111846\n",
      " 0.0594107  0.05943476 0.06316296 0.05017438 0.06008419 0.044546\n",
      " 0.05345364 0.05219891 0.05118345 0.0504397 ]\n",
      "4 day output [[0.049932]]\n",
      "5 day input [0.62871918 0.62388455 0.64589296 0.6643175  0.64875526 0.66025256\n",
      " 0.63328924 0.65289236 0.66015634 0.65849669 0.65849669 0.68192423\n",
      " 0.6791341  0.70001203 0.7039567  0.70556825 0.70131088 0.71651233\n",
      " 0.72293446 0.711365   0.70948888 0.72901984 0.71595911 0.71858088\n",
      " 0.67598316 0.67377029 0.69488876 0.67990379 0.69710162 0.70816597\n",
      " 0.80505111 0.83081179 0.83781118 0.8416356  0.87858088 0.85173782\n",
      " 0.86727601 0.83502105 0.86999399 0.88923632 0.88825015 0.8853638\n",
      " 0.89455201 0.89594708 0.92064943 0.97929044 0.99360192 0.98366807\n",
      " 1.         0.98544799 0.9834997  0.09308479 0.10544799 0.09876127\n",
      " 0.07345761 0.07365003 0.05407096 0.06489477 0.0556825  0.05209862\n",
      " 0.06016837 0.06061335 0.0524113  0.04810583 0.03968731 0.04748046\n",
      " 0.05164161 0.04036079 0.04300661 0.0527721  0.05921828 0.05712568\n",
      " 0.06126278 0.06361996 0.05455201 0.06292243 0.05488876 0.05950692\n",
      " 0.05924233 0.06405292 0.08192423 0.07398677 0.07420325 0.07304871\n",
      " 0.06898376 0.06167168 0.06535177 0.06381239 0.06111846 0.0594107\n",
      " 0.05943476 0.06316296 0.05017438 0.06008419 0.044546   0.05345364\n",
      " 0.05219891 0.05118345 0.0504397  0.049932  ]\n",
      "5 day output [[0.04960523]]\n",
      "6 day input [0.62388455 0.64589296 0.6643175  0.64875526 0.66025256 0.63328924\n",
      " 0.65289236 0.66015634 0.65849669 0.65849669 0.68192423 0.6791341\n",
      " 0.70001203 0.7039567  0.70556825 0.70131088 0.71651233 0.72293446\n",
      " 0.711365   0.70948888 0.72901984 0.71595911 0.71858088 0.67598316\n",
      " 0.67377029 0.69488876 0.67990379 0.69710162 0.70816597 0.80505111\n",
      " 0.83081179 0.83781118 0.8416356  0.87858088 0.85173782 0.86727601\n",
      " 0.83502105 0.86999399 0.88923632 0.88825015 0.8853638  0.89455201\n",
      " 0.89594708 0.92064943 0.97929044 0.99360192 0.98366807 1.\n",
      " 0.98544799 0.9834997  0.09308479 0.10544799 0.09876127 0.07345761\n",
      " 0.07365003 0.05407096 0.06489477 0.0556825  0.05209862 0.06016837\n",
      " 0.06061335 0.0524113  0.04810583 0.03968731 0.04748046 0.05164161\n",
      " 0.04036079 0.04300661 0.0527721  0.05921828 0.05712568 0.06126278\n",
      " 0.06361996 0.05455201 0.06292243 0.05488876 0.05950692 0.05924233\n",
      " 0.06405292 0.08192423 0.07398677 0.07420325 0.07304871 0.06898376\n",
      " 0.06167168 0.06535177 0.06381239 0.06111846 0.0594107  0.05943476\n",
      " 0.06316296 0.05017438 0.06008419 0.044546   0.05345364 0.05219891\n",
      " 0.05118345 0.0504397  0.049932   0.04960523]\n",
      "6 day output [[0.04940876]]\n",
      "7 day input [0.64589296 0.6643175  0.64875526 0.66025256 0.63328924 0.65289236\n",
      " 0.66015634 0.65849669 0.65849669 0.68192423 0.6791341  0.70001203\n",
      " 0.7039567  0.70556825 0.70131088 0.71651233 0.72293446 0.711365\n",
      " 0.70948888 0.72901984 0.71595911 0.71858088 0.67598316 0.67377029\n",
      " 0.69488876 0.67990379 0.69710162 0.70816597 0.80505111 0.83081179\n",
      " 0.83781118 0.8416356  0.87858088 0.85173782 0.86727601 0.83502105\n",
      " 0.86999399 0.88923632 0.88825015 0.8853638  0.89455201 0.89594708\n",
      " 0.92064943 0.97929044 0.99360192 0.98366807 1.         0.98544799\n",
      " 0.9834997  0.09308479 0.10544799 0.09876127 0.07345761 0.07365003\n",
      " 0.05407096 0.06489477 0.0556825  0.05209862 0.06016837 0.06061335\n",
      " 0.0524113  0.04810583 0.03968731 0.04748046 0.05164161 0.04036079\n",
      " 0.04300661 0.0527721  0.05921828 0.05712568 0.06126278 0.06361996\n",
      " 0.05455201 0.06292243 0.05488876 0.05950692 0.05924233 0.06405292\n",
      " 0.08192423 0.07398677 0.07420325 0.07304871 0.06898376 0.06167168\n",
      " 0.06535177 0.06381239 0.06111846 0.0594107  0.05943476 0.06316296\n",
      " 0.05017438 0.06008419 0.044546   0.05345364 0.05219891 0.05118345\n",
      " 0.0504397  0.049932   0.04960523 0.04940876]\n",
      "7 day output [[0.04930322]]\n",
      "8 day input [0.6643175  0.64875526 0.66025256 0.63328924 0.65289236 0.66015634\n",
      " 0.65849669 0.65849669 0.68192423 0.6791341  0.70001203 0.7039567\n",
      " 0.70556825 0.70131088 0.71651233 0.72293446 0.711365   0.70948888\n",
      " 0.72901984 0.71595911 0.71858088 0.67598316 0.67377029 0.69488876\n",
      " 0.67990379 0.69710162 0.70816597 0.80505111 0.83081179 0.83781118\n",
      " 0.8416356  0.87858088 0.85173782 0.86727601 0.83502105 0.86999399\n",
      " 0.88923632 0.88825015 0.8853638  0.89455201 0.89594708 0.92064943\n",
      " 0.97929044 0.99360192 0.98366807 1.         0.98544799 0.9834997\n",
      " 0.09308479 0.10544799 0.09876127 0.07345761 0.07365003 0.05407096\n",
      " 0.06489477 0.0556825  0.05209862 0.06016837 0.06061335 0.0524113\n",
      " 0.04810583 0.03968731 0.04748046 0.05164161 0.04036079 0.04300661\n",
      " 0.0527721  0.05921828 0.05712568 0.06126278 0.06361996 0.05455201\n",
      " 0.06292243 0.05488876 0.05950692 0.05924233 0.06405292 0.08192423\n",
      " 0.07398677 0.07420325 0.07304871 0.06898376 0.06167168 0.06535177\n",
      " 0.06381239 0.06111846 0.0594107  0.05943476 0.06316296 0.05017438\n",
      " 0.06008419 0.044546   0.05345364 0.05219891 0.05118345 0.0504397\n",
      " 0.049932   0.04960523 0.04940876 0.04930322]\n",
      "8 day output [[0.04926094]]\n",
      "9 day input [0.64875526 0.66025256 0.63328924 0.65289236 0.66015634 0.65849669\n",
      " 0.65849669 0.68192423 0.6791341  0.70001203 0.7039567  0.70556825\n",
      " 0.70131088 0.71651233 0.72293446 0.711365   0.70948888 0.72901984\n",
      " 0.71595911 0.71858088 0.67598316 0.67377029 0.69488876 0.67990379\n",
      " 0.69710162 0.70816597 0.80505111 0.83081179 0.83781118 0.8416356\n",
      " 0.87858088 0.85173782 0.86727601 0.83502105 0.86999399 0.88923632\n",
      " 0.88825015 0.8853638  0.89455201 0.89594708 0.92064943 0.97929044\n",
      " 0.99360192 0.98366807 1.         0.98544799 0.9834997  0.09308479\n",
      " 0.10544799 0.09876127 0.07345761 0.07365003 0.05407096 0.06489477\n",
      " 0.0556825  0.05209862 0.06016837 0.06061335 0.0524113  0.04810583\n",
      " 0.03968731 0.04748046 0.05164161 0.04036079 0.04300661 0.0527721\n",
      " 0.05921828 0.05712568 0.06126278 0.06361996 0.05455201 0.06292243\n",
      " 0.05488876 0.05950692 0.05924233 0.06405292 0.08192423 0.07398677\n",
      " 0.07420325 0.07304871 0.06898376 0.06167168 0.06535177 0.06381239\n",
      " 0.06111846 0.0594107  0.05943476 0.06316296 0.05017438 0.06008419\n",
      " 0.044546   0.05345364 0.05219891 0.05118345 0.0504397  0.049932\n",
      " 0.04960523 0.04940876 0.04930322 0.04926094]\n",
      "9 day output [[0.04926239]]\n",
      "[[0.05345363914966583], [0.05219890922307968], [0.05118345096707344], [0.05043970048427582], [0.04993199557065964], [0.04960523173213005], [0.04940875619649887], [0.04930322244763374], [0.0492609441280365], [0.0492623895406723]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "lst_output=[]\n",
    "n_steps=100\n",
    "i=0\n",
    "while(i<10):\n",
    "    \n",
    "    if(len(temp_input)>100):\n",
    "        #print(temp_input)\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        print(\"{} day input {}\".format(i,x_input))\n",
    "        x_input=x_input.reshape(1,-1)\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "        #print(x_input)\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "    else:\n",
    "        x_input = x_input.reshape((1, n_steps,1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        print(yhat[0])\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        print(len(temp_input))\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "    \n",
    "\n",
    "print(lst_output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deep learning mini project LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
